{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ShopifyAnalysis_Handin_Version.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shopify Analysis Hand-In Notebook\n",
        "\n",
        "This notebook contains sample code and code and text fields relating to the practical project of Principles of Machine Learning. \n",
        "\n",
        "The targeted dataset consists of a scraped dump of shopify store descriptions and associated meta-data (https://www.kaggle.com/datasets/shopgram/shopify-stores-by-shopgramio).\n",
        "\n"
      ],
      "metadata": {
        "id": "ek14lRPlr8mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration\n",
        "\n",
        "The following cells contain code to connect a Google Drive folder and to explore the dataset. "
      ],
      "metadata": {
        "id": "6mAQqTkgs3Xp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ul1hNt2bH-5"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "The first step consist of loading the dataset we downloaded from Kaggle. \n",
        "It is available on the teams channel for the practical work. \n",
        "\n",
        "The lines below import the pandas library with the alias `pd`, and loads the `CSV` file into a `dataframe`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O stores_data_UTF8.csv  https://drive.switch.ch/index.php/s/mvOKN47CFmlSi2S/download"
      ],
      "metadata": {
        "id": "205TvoSQ_iF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRVFtLLgZAGi"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./stores_data_UTF8.csv', lineterminator=\"\\n\")\n",
        "from google.colab.data_table import DataTable\n",
        "DataTable.max_columns = 100\n",
        "pd.set_option('display.max_columns', 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4P65l-bXHT"
      },
      "source": [
        "## Dataframe Exploration\n",
        "\n",
        "It is usually a good idea to explore the `dataframe` after loading in order to better understand the data, and to make sure the loading worked as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMgUIkwfkzJW"
      },
      "source": [
        "# This shows us the column labels in the dataframe.\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVrYYOtPaWmB"
      },
      "source": [
        "# The value counts function is a good way to get an overview of the values contained in a column by frequency (the most common and least common values are shown below).\n",
        "df['store_title'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTexsJffnZ-z"
      },
      "source": [
        "# The describe function prints some basic descriptive stats on the columns in the dataframe (e.g. number of values, number of unique values. )\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OKcsFVcdrCn"
      },
      "source": [
        "# The tail() function allows us to look at the last entries in the dataframe. The head() function shows us the top entries in the dataframe.\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Dataset Creation"
      ],
      "metadata": {
        "id": "45oELKWBtQXg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr9P0f5Xd_yo"
      },
      "source": [
        "## Data Pre-Processing \n",
        "\n",
        "It is often necessary to do some form of pre-procesing of the data.\n",
        "\n",
        "This can become necesary in order to handle `null` values, deal with wrong data types, or make sure that data is encoded consistently in the same format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQS6VYqpfMgY"
      },
      "source": [
        "print(f\"There are {df['store_title'].isnull().sum()} null values in the 'store_title' column and {df['store_description'].isnull().sum()} null values in the 'store_description' column\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VurJlP-5n_bg"
      },
      "source": [
        "# This replaces the null values with an empty string.\n",
        "df['store_labels'].fillna(\"\", inplace=True)\n",
        "df['store_description'].fillna(\"\", inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm7ErUWygYP4"
      },
      "source": [
        "## Semi-Supervised Label Creation\n",
        "\n",
        "The first step to build a classifier for our targeted categories consists of creating labels. \n",
        "\n",
        "In order to train our machine learning system that is expected to be able to classify shops based on their `store_description` or the content of the `store_collections` field, we need to label the rows in the dataframe.\n",
        "\n",
        "The cell below shows a simple approach to do this based on using the existing `store_labels`.\n",
        "* We define our set of `ml categories` and assign an integer to each of these categories\n",
        "* We collect keywords that we believe to match each of our target categories\n",
        "* If the `store_labels` of a shop contain one of these keywords we will assign the respective integer label of the category.\n",
        "* Finally, we assign a label representing all unmatched rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLewCqeqk8MY"
      },
      "source": [
        "\n",
        "# In this example I assign the following ints to each category\n",
        "\n",
        "\n",
        "# I use the following (very limited) selection of keywords for the two categories\n",
        "df = df.assign(ml_labels=100)\n",
        "label_class_a = ['dungeons and dragons', 'shadowrun', 'board games', 'dice']\n",
        "label_class_b = ['case', 'iphone', 'samsung', 'android']\n",
        "\n",
        "\n",
        "# In order to match these categories I use the following code. If any of the \n",
        "# listed keywords is contained in the store_labels field it will be labeled with \n",
        "# the category in the new column ml_labels\n",
        "df.loc[df.store_description.str.contains('|'.join(label_class_a), case = False, na = False), 'ml_labels'] = 1\n",
        "df.loc[df.store_description.str.contains('|'.join(label_class_b), case = False, na = False), 'ml_labels'] = 2\n",
        "\n",
        "\n",
        "# After having labeled all our matching rows we label all remaining rows with an\n",
        "# int representing all the other categories.\n",
        "df['ml_labels'] = df['ml_labels'].where((df['ml_labels'].isin([1,2])), other=100)\n",
        "\n",
        "# To have a quick check we can use the head() or tail() command and see if we\n",
        "# have matches.\n",
        "df.groupby('ml_labels').sample(n=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmjUxh_j3sjl"
      },
      "source": [
        "# To see overall results of your labeling efforts, you can use the value_counts() function again.\n",
        "df['ml_labels'].value_counts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semi-supervised approaches to label machine learning training data are a rapidly developing fields and are big business. \n",
        "\n",
        "One of the fastest growing companies specialised on this is https://snorkel.ai/ ; a spin-off from Stanford University. \n"
      ],
      "metadata": {
        "id": "eqMdADEKtgKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysing your Automatically Labeled Data"
      ],
      "metadata": {
        "id": "egT-8J8muD_A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ14KE96FG98"
      },
      "source": [
        "# If you want to look at the individual values for a label you can use the command below.\n",
        "df.loc[df['ml_labels'] == 2.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk86x4gokz4Z"
      },
      "source": [
        "# If you want to see the full output you can write it out to a file.\n",
        "# This will write to the temporary file space of a Google collab notebook if you run the notebook there.\n",
        "df.loc[df['ml_labels'] == 1.0].to_csv(r'matched_label_1.txt', header=None, index=None, sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBkb_u3CrL8v"
      },
      "source": [
        "## Creating the Training Data Structures\n",
        "\n",
        "Now that we have labelled data, it is time to create the training set. \n",
        "In order to train our classifier we need the set of:\n",
        "* Samples `X` and\n",
        "* the set of labels for these samples `y`\n",
        "\n",
        "Both `X` and `y` have to be encoded in numerical form. \n",
        "In order to transform the text of the descriptions we make use of the `CountVectorizer`.\n",
        "\n",
        "For the vector holding the labels we just have to ensure it is an `int`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS0OwLcWajhA"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "X = count_vect.fit_transform(df['store_description'].apply(lambda x: np.str_(x)))\n",
        "y = df['ml_labels'].astype(int)\n",
        "X.shape\n",
        "y.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV90oG7oGnlB"
      },
      "source": [
        "# This shows you how the samples have been transformed from text into a numerical \n",
        "# represenation. If you want to see the full output you have to write it to a file.\n",
        "X[1:10].todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Train and Test Sets\n",
        "Take the input X and y and create a train and test version that we can use to create the first classifiers. "
      ],
      "metadata": {
        "id": "oOWJKRclNp-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=54)"
      ],
      "metadata": {
        "id": "2UDcpomuNoTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ7psG2esbb7"
      },
      "source": [
        "# Training and Scoring the Classifier Model\n",
        "\n",
        "After the all the hard work we have done above, the actual training and scoring of the classifier is very simple. \n",
        "\n",
        "As shown below, all we have to do is to call the `fit()` method with the input of the samples `X` and their labels `y`. \n",
        "\n",
        "Afterwards we measure by calling the `score()` method. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_4zP1lrbheU"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdydiEJ9s4gY"
      },
      "source": [
        "# Practical Part Deliverables\n",
        "\n",
        "As part of the grading for Principles of  Machine Learning each group has to complete the following steps:\n",
        "\n",
        "### 1. Create a labelled dataset\n",
        "\n",
        "Following the schema shown above, map your chosen categories to keywords and create labels in an ml_labels column.\n",
        "\n",
        "After you have finished this process export your resulting dataframe as a `csv` file (example code for this is contained in this notebook). \n",
        "\n",
        "### 2. Train a classifier\n",
        "\n",
        "If you want you can use the same sample code contained in this notebook to train your first classifier based on your labelling. You can also look at the sci-kit learn documentation and try your hand at other classifiers (this is not required from a grading perspective).\n",
        "\n",
        "### 3. Measure correctly\n",
        "\n",
        "In the above example code we train and measure on the test set. This is already a good approach, and depending on the size of your test data can give meaningful results.\n",
        "Explore another way to measure by applying cross-validation when measuring. \n",
        "\n",
        "### 4. Interprete the process and your results\n",
        "\n",
        "Provide a short (not more than 10 sentences) written interpretation of the observed result and the process that we have used to create our classifier. \n",
        "\n",
        "Do you see any potential problems in the semi_supervised keyword labeling shortcut we have used?\n",
        "\n",
        "How would you interpret the observed results? Could the classifier be used for the discussed purpose of doing some preliminary competitive analysis (e.g. what is my competition in this area, how many shops exist that serve the products I target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ14qwzEw0S6"
      },
      "source": [
        "## Interpretation of the process and our results\n",
        "\n",
        "Write your input below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Measuring Correctly\n",
        "\n",
        "Provide your baseline measurements:\n",
        "- Train/test split based measurements:\n",
        "    - Split Ratio:\n",
        "    - Accuracy:\n",
        "- Cross-Validation measurements:\n",
        "    - Number of Folds:\n",
        "    - Accuracy:\n",
        "\n",
        "\n",
        "## 4. Interprete the process and your results\n",
        "\n",
        "- What is your general interpretation of the results? Are they good, bad, mediocre? Describe briefly.\n",
        "\n",
        "...\n",
        "\n",
        "- Do you see any potential problems in the semi_supervised keyword labeling shortcut that was used?\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "- How would you interpret the observed results? Could the classifier be used for your intended purpose? \n",
        "\n",
        "...\n",
        "\n",
        "\n",
        "- What could be done to improve the performance of your classifier?\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "aXi4IKGtPOxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hand-in Procedure\n",
        "\n",
        "The hand-in for each group consists of the CSV file containing your labelled version of the dataframe, and your version of this notebook.\n",
        "\n",
        "Place the notebook file and the CSV file in a Zip file and upload it to the teams channel for Part II of the practical project. \n",
        "\n",
        "The name of the zip file should contain the last names of the group members. \n",
        "\n"
      ],
      "metadata": {
        "id": "07foL5nzu8Kt"
      }
    }
  ]
}